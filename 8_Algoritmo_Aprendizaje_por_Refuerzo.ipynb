{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# José Luis Padilla Valenzuela\n",
    "\n",
    "## Algoritmo de Aprendizaje por refuerzo\n",
    "\n",
    "\n",
    "* Vamos a resolver un problema con Aprendizaje por refuerzo usando los algoritmos del Q-Learning y SARSA-Learning (implementados en este Notebook).\n",
    "\n",
    "\n",
    "* El problema que queremos resolver es el de encontrar el camino que nos suponga una mayor recompensa (el más corto) desde un estado inicial $[0,0]$ hasta el estado final $[4,4]$, pudiendo realizar 4 tipos de acciones:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/007_RL.png\" style=\"width: 250px;\"/>\n",
    "\n",
    "\n",
    "* Como vemos en esta imagen (el entorno) tenemos dos estados en lo que podemos recibir recompensa bien sea negativa (-100) o positiva (+100).\n",
    "\n",
    "* Para resolver este problema vamos a realizar lo siguiente:\n",
    "<span></span><br>\n",
    "    1. [Definición del entorno](#M1)\n",
    "<span></span><br>\n",
    "    2. [Implementación de un algoritmo de toma aleatoria de acciones](#M2)\n",
    "<span></span><br>\n",
    "    3. [Ejecución: Entorno - Agente](#M3)\n",
    "<span></span><br>\n",
    "    4. [Q-Learner: Implementación y Ejecución](#M4)\n",
    "<span></span><br>\n",
    "    5. [SARSA-Learner: Implementación y Ejecución](#M5)\n",
    "<span></span><br>\n",
    "    6. [Ejecuciones a realizar por el alumno](#M6)\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M1\">1.- Definición del entorno</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerias necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Formato de los decimales en Pandas y la semilla del Random\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.random.seed(5)\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    def __init__(self, action_penalty = -1.0):\n",
    "        \"\"\"\n",
    "        Clase que representa y controla en entorno\n",
    "        \n",
    "        :param step_penalty:     Factor de descuento del Reward por acción tomada\n",
    "        \"\"\"\n",
    "        self.actions = {'Arriba': [-1, 0],\n",
    "                        'Abajo': [1, 0],\n",
    "                        'Izquierda': [0, -1],\n",
    "                        'Derecha': [0, 1]}\n",
    "        self.rewards = [[0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, 0.0, 0.0],\n",
    "                        [0.0, 0.0, -100.0, 100.0]]\n",
    "        self.action_penalty = action_penalty\n",
    "        self.state = [0, 0]\n",
    "        self.final_state = [3, 3]\n",
    "        self.total_reward = 0.0\n",
    "        self.actions_done = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Método que reinicia las variables del entorno y devuelve es estado inicial\n",
    "       \n",
    "        :return:    state\n",
    "        \"\"\"\n",
    "        self.total_reward = 0.0\n",
    "        self.state = [0, 0]\n",
    "        self.actions_done = []\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Método que ejecuta una acción determinada del conjunto de acciones {Arriba, Abajo, Izquierda, Derecha}\n",
    "        para guiar al agente en el entorno.\n",
    "        \n",
    "        :param action:    Acción a ejecutar\n",
    "        :return:          (state, reward, is_final_state)\n",
    "        \"\"\"\n",
    "        self.__apply_action(action)\n",
    "        self.actions_done.append(self.state[:])\n",
    "        is_final_state = np.array_equal(self.state, self.final_state)\n",
    "        reward = self.rewards[self.state[0]][self.state[1]] + self.action_penalty\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward, is_final_state\n",
    "\n",
    "    def __apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Método que calcula el nuevo estado a partir de la acción a ejecutar\n",
    "        \n",
    "        :param action:    Acción a ejecutar\n",
    "        \"\"\"\n",
    "        self.state[0] += self.actions[action][0]\n",
    "        self.state[1] += self.actions[action][1]\n",
    "\n",
    "        # Si me salgo del tablero por arriba o por abajo, me quedo en la posición que estaba\n",
    "        if self.state[0] < 0:\n",
    "            self.state[0] = 0\n",
    "        elif self.state[0] > len(self.rewards) - 1:\n",
    "            self.state[0] -= 1\n",
    "\n",
    "        # Si me salgo del tablero por los lados, me quedo en la posición que estaba\n",
    "        if self.state[1] < 0:\n",
    "            self.state[1] = 0\n",
    "        elif self.state[1] > len(self.rewards[0]) - 1:\n",
    "            self.state[1] -= 1\n",
    "\n",
    "    def print_path_episode(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el camino seguido por el agente \n",
    "        \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        path = [['-' for _ in range(len(self.rewards))] for _ in range(len(self.rewards[0]))]\n",
    "        path[0][0] = '0'\n",
    "        for index, step in enumerate(self.actions_done):\n",
    "            path[step[0]][step[1]] = str(index + 1)\n",
    "\n",
    "        print(pd.DataFrame(data = np.array([np.array(xi) for xi in path]),\n",
    "                           index = [\"x{}\".format(str(i)) for i in range(len(path))],\n",
    "                           columns = [\"y{}\".format(str(i)) for i in range(len(path[0]))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M2\">2.- Implementación de un algoritmo de toma aleatoria de acciones</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "\n",
    "    def __init__(self, environment, learning_rate = 0.1, discount_factor = 0.1, ratio_exploration = 0.05):\n",
    "        \"\"\"\n",
    "        Clase que implementa un algoritmo de aprendiza por refuerzo\n",
    "        Esta clase implementa un algoritmo de selección aleatoria de acciones\n",
    "        \n",
    "        :param environment:         Entorno en el que tomar las acciones\n",
    "        :param learning_rate:       Factor de aprendizaje\n",
    "        :param discount_factor:     Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "        :param ratio_exploration:   Ratio de exploración\n",
    "        \"\"\"\n",
    "        self.environment = environment\n",
    "        self.q_table = [[[0.0 for _ in self.environment.actions]\n",
    "                         for _ in range(len(self.environment.rewards))]\n",
    "                        for _ in range(len(self.environment.rewards[0]))]\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.ratio_exploration = ratio_exploration\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'random'\n",
    "\n",
    "    def get_next_action(self, state):\n",
    "        \"\"\"\n",
    "        Método que selecciona la siguiente acción a tomar:\n",
    "        \n",
    "            Aleatoria    -> si el ratio de exploración es inferior al umbral\n",
    "            Mejor Acción -> si el ratio de exploración es superior al umbral\n",
    "        :param state:       Estado del agente\n",
    "        :return:            next_action\n",
    "        \"\"\"\n",
    "\n",
    "        if np.random.uniform() < self.ratio_exploration:\n",
    "            # Selecciono una opción al azar\n",
    "            next_action = np.random.choice(list(self.environment.actions))\n",
    "        else:\n",
    "            # Selecciono la acción que me de mayor valor. Si hay empate, selecciono una al azar\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                self.q_table[state[0]][state[1]] == np.array(self.q_table[state[0]][state[1]]).max()\n",
    "            ))\n",
    "            next_action = list(self.environment.actions)[idx_action]\n",
    "\n",
    "        return next_action\n",
    "\n",
    "    def update(self, ** kwargs):\n",
    "        \"\"\"\n",
    "        Actualiza la Q-Table\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def print_q_table(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla la Q-Table\n",
    "        \"\"\"\n",
    "        print(\"Actions = {}\".format(' - '.join([k for k, v in self.environment.actions.items()])))\n",
    "        for row in self.q_table:\n",
    "            for col in row:\n",
    "                print('[', end = '')\n",
    "                for val in col:\n",
    "                    print('{:0.2f}'.format(val), end = ' ')\n",
    "                print('] ', end = '')\n",
    "            print()\n",
    "\n",
    "    def print_best_values_states(self):\n",
    "        \"\"\"\n",
    "        Método que imprime por pantalla el valor de la mejor opción a realizar en cada uno de los estados\n",
    "        \"\"\"\n",
    "        best = [[max(vi) for vi in row] for row in self.q_table]\n",
    "        print(pd.DataFrame(data = np.array([np.array(xi) for xi in best]),\n",
    "                           index = [\"x{}\".format(str(i)) for i in range(len(best))],\n",
    "                           columns = [\"y{}\".format(str(i)) for i in range(len(best[0]))]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M3\">3.- Ejecución: Entorno - Agente</a>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_agent(learner = Learner, num_episodes = 10, learning_rate = 0.1, discount_factor = 0.1, ratio_exploration = 0.05,\n",
    "              verbose = False):\n",
    "    \"\"\"\n",
    "    Método que ejecuta el proceso de aprendizaje del agente en un entorno\n",
    "    \n",
    "    :param learner:              Algoritmo de Aprendizaje\n",
    "    :param num_episodes:         Número de veces que se ejecuta (o aprende) el agente en el entorno\n",
    "    :param learning_rate:        Factor de Aprendizaje\n",
    "    :param discount_factor:      Factor de descuento (0=Estrategia a corto plazo, 1=Estrategia a largo plazo)\n",
    "    :param ratio_exploration:    Ratio de exploración\n",
    "    :param verbose:              Boolean, si queremos o no imprimir por pantalla información del proceso\n",
    "    :return:                     (episodes_list, best_episode)\n",
    "    \"\"\"\n",
    "\n",
    "    # Instanciamos el entorno\n",
    "    environment = Environment()\n",
    "\n",
    "    # Instanciamos el método de aprendizaje\n",
    "    learner = learner(environment = environment,\n",
    "                      learning_rate = learning_rate,\n",
    "                      discount_factor = discount_factor,\n",
    "                      ratio_exploration = ratio_exploration)\n",
    "\n",
    "    # Variables para guardar la información de los episodios\n",
    "    episodes_list = []\n",
    "    best_reward = float('-inf')\n",
    "    best_episode = None\n",
    "\n",
    "    for n_episode in range(0, num_episodes):\n",
    "        state = environment.reset()\n",
    "        reward = None\n",
    "        is_final_state = None\n",
    "        num_steps_episode = 0\n",
    "        while (is_final_state != True):\n",
    "            old_state = state[:]\n",
    "            next_action = learner.get_next_action(state=state)\n",
    "            state, reward, is_final_state = environment.step(next_action)\n",
    "            next_post_action = learner.get_next_action(state) if learner.name == 'SARSA' else None\n",
    "            learner.update(environment = environment,\n",
    "                           old_state = old_state,\n",
    "                           action_taken = next_action,\n",
    "                           reward_action_taken = reward,\n",
    "                           new_state = state,\n",
    "                           new_action = next_post_action,\n",
    "                           is_final_state = is_final_state)\n",
    "            num_steps_episode += 1\n",
    "\n",
    "        # Guardamos la información del episodio\n",
    "        episodes_list.append([n_episode + 1, num_steps_episode, environment.total_reward])\n",
    "\n",
    "        # Guardamos el mejor episodio\n",
    "        if environment.total_reward >= best_reward:\n",
    "            best_reward = environment.total_reward\n",
    "            best_episode = {'num_episode': n_episode + 1,\n",
    "                            'episode': deepcopy(environment),\n",
    "                            'learner': deepcopy(learner)}\n",
    "\n",
    "        if verbose:\n",
    "            # Imprimimos la información de los episodios\n",
    "            print('EPISODIO {} - Numero de acciones: {} - Reward: {}'\n",
    "                  .format(n_episode + 1, num_steps_episode, environment.total_reward))\n",
    "\n",
    "    return episodes_list, best_episode\n",
    "\n",
    "\n",
    "def print_process_info(episodes_list, best_episode, print_best_episode_info = True,\n",
    "                       print_q_table = True, print_best_values_states = True,\n",
    "                       print_steps = True, print_path = True):\n",
    "    \"\"\"\n",
    "    Método que imprime por pantalla los resultados de la ejecución\n",
    "    \"\"\"\n",
    "    if print_best_episode_info:\n",
    "        print('\\nMEJOR (ÚLTIMO) EPISODIO:\\n\\EPISODIO {}\\n\\tNumero de acciones: {}\\n\\tReward: {}'\n",
    "              .format(best_episode['num_episode'],\n",
    "                      len(best_episode['episode'].actions_done),\n",
    "                      best_episode['episode'].total_reward))\n",
    "\n",
    "    if print_q_table:\n",
    "        print('\\nQ_TABLE:')\n",
    "        best_episode['learner'].print_q_table()\n",
    "\n",
    "    if print_best_values_states:\n",
    "        print('\\nBEST Q_TABLE VALUES:')\n",
    "        best_episode['learner'].print_best_values_states()\n",
    "\n",
    "    if print_steps:\n",
    "        print('\\nPasos: \\n   {}'.format(best_episode['episode'].actions_done))\n",
    "\n",
    "    if print_path:\n",
    "        print('\\nPATH:')\n",
    "        best_episode['episode'].print_path_episode()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución:\n",
    "\n",
    "* En este primer ejemplo de toma de decisiones en el que se usa una política aleatoria, podemos observar como el agente no aprende nada y los movimientos son aleatorios en los 10 episodios que ejecutamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 91 - Reward: -391.0\n",
      "EPISODIO 2 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 3 - Numero de acciones: 17 - Reward: -117.0\n",
      "EPISODIO 4 - Numero de acciones: 60 - Reward: -360.0\n",
      "EPISODIO 5 - Numero de acciones: 312 - Reward: -2012.0\n",
      "EPISODIO 6 - Numero de acciones: 148 - Reward: -648.0\n",
      "EPISODIO 7 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 8 - Numero de acciones: 216 - Reward: -616.0\n",
      "EPISODIO 9 - Numero de acciones: 52 - Reward: 48.0\n",
      "EPISODIO 10 - Numero de acciones: 45 - Reward: 55.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 7\n",
      "\tNumero de acciones: 19\n",
      "\tReward: 81.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "[0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "     y0   y1   y2   y3\n",
      "x0 0.00 0.00 0.00 0.00\n",
      "x1 0.00 0.00 0.00 0.00\n",
      "x2 0.00 0.00 0.00 0.00\n",
      "x3 0.00 0.00 0.00 0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [1, 1], [1, 0], [0, 0], [1, 0], [1, 0], [1, 1], [1, 2], [2, 2], [2, 3], [1, 3], [2, 3], [2, 2], [2, 3], [2, 2], [2, 1], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0  y1  y2  y3\n",
      "x0  4   -   -   -\n",
      "x1  6   7   8  11\n",
      "x2  -  16  17  18\n",
      "x3  -   -   -  19\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner = Learner, \n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list, \n",
    "                    best_episode = best_episode)"
   ]
  },
  {
   "attachments": {
    "Q-learning.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M4\">4.- Q-Learner: Implementación y Ejecución</a>\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "![Q-learning.png](attachment:Q-learning.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'QLearner'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, is_final_state, ** kwargs):\n",
    "        \"\"\"\n",
    "        Método que implementa el Algoritmo de aprendizaje del Q-Learning\n",
    "        \n",
    "        :param environment:          Entorno en el que tomar las acciones\n",
    "        :param old_state:            Estado actual\n",
    "        :param action_taken:         Acción a realizar\n",
    "        :param reward_action_taken:  Recompensa obtenida por la acción tomada\n",
    "        :param new_state:            Nuevo estado al que se mueve el agente\n",
    "        :param is_final_state:       Boolean. Devuelvel True si el agente llega al estado final; si no, False\n",
    "        :param kwargs: \n",
    "        \"\"\"\n",
    "        # Obtengo el identificador de la acción\n",
    "        idx_action_taken = list(environment.actions).index(action_taken)\n",
    "\n",
    "        # Obtengo el valor de la acción tomada\n",
    "        actual_q_value_options = self.q_table[old_state[0]][old_state[1]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self.q_table[new_state[0]][new_state[1]]\n",
    "        future_max_q_value = reward_action_taken + self.discount_factor * max(future_q_value_options)\n",
    "        if is_final_state:\n",
    "            # Reward máximo si llego a la posición final\n",
    "            future_max_q_value = reward_action_taken\n",
    "\n",
    "        self.q_table[old_state[0]][old_state[1]][idx_action_taken] = \\\n",
    "            actual_q_value + self.learning_rate * (future_max_q_value - actual_q_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a corto plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a corto plazo (discount_factor=0.1) pero los movimientos los realiza hacia el estado que mayor recompensa parcial le de y no tiene muy encuenta la recompensa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 41 - Reward: -41.0\n",
      "EPISODIO 2 - Numero de acciones: 52 - Reward: -52.0\n",
      "EPISODIO 3 - Numero de acciones: 29 - Reward: 71.0\n",
      "EPISODIO 4 - Numero de acciones: 18 - Reward: 82.0\n",
      "EPISODIO 5 - Numero de acciones: 35 - Reward: 65.0\n",
      "EPISODIO 6 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 7 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 8 - Numero de acciones: 13 - Reward: 87.0\n",
      "EPISODIO 9 - Numero de acciones: 43 - Reward: 57.0\n",
      "EPISODIO 10 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 11 - Numero de acciones: 21 - Reward: -21.0\n",
      "EPISODIO 12 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 13 - Numero de acciones: 24 - Reward: 76.0\n",
      "EPISODIO 14 - Numero de acciones: 36 - Reward: 64.0\n",
      "EPISODIO 15 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 16 - Numero de acciones: 41 - Reward: 59.0\n",
      "EPISODIO 17 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 18 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 19 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 20 - Numero de acciones: 43 - Reward: 57.0\n",
      "EPISODIO 21 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 22 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 23 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 24 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 25 - Numero de acciones: 28 - Reward: 72.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 23\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-1.00 -1.00 -1.00 -1.00 ] [-0.81 -0.83 -0.83 -0.83 ] [-0.59 -0.58 -0.60 -0.58 ] [-0.35 -0.38 -0.35 -0.42 ] \n",
      "[-0.86 -0.85 -0.84 -0.83 ] [-0.69 -0.71 -0.69 -0.70 ] [-0.42 -0.42 -0.43 -0.42 ] [-0.10 3.11 -0.10 -0.10 ] \n",
      "[-0.69 -0.68 -0.68 -0.71 ] [-0.54 -0.54 -0.54 -0.52 ] [-0.19 -10.10 -0.10 4.45 ] [-0.10 89.25 0.00 -0.10 ] \n",
      "[-0.59 -0.59 -0.59 -0.63 ] [-0.59 -0.54 -0.54 -19.19 ] [-0.10 0.00 -0.10 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -1.00 -0.81 -0.58 -0.35\n",
      "x1 -0.83 -0.69 -0.42  3.11\n",
      "x2 -0.68 -0.52  4.45 89.25\n",
      "x3 -0.59 -0.54  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [2, 0], [2, 1], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  -  -  -\n",
      "x1  1  -  -  -\n",
      "x2  2  3  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner = QLearner,\n",
    "                                        num_episodes = 25,\n",
    "                                        learning_rate = 0.1,\n",
    "                                        discount_factor = 0.1,\n",
    "                                        ratio_exploration = 0.05,\n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list,\n",
    "                    best_episode = best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a largo plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a largo plazo (discount_factor=0.9), realizando movimientos que le den una recompesa final mayor y no asi una recompensa parcial mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 20 - Reward: -120.0\n",
      "EPISODIO 2 - Numero de acciones: 41 - Reward: 59.0\n",
      "EPISODIO 3 - Numero de acciones: 21 - Reward: 79.0\n",
      "EPISODIO 4 - Numero de acciones: 54 - Reward: 46.0\n",
      "EPISODIO 5 - Numero de acciones: 23 - Reward: 77.0\n",
      "EPISODIO 6 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 7 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 8 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 9 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 10 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 11 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 12 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 13 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 14 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 15 - Numero de acciones: 13 - Reward: 87.0\n",
      "EPISODIO 16 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 17 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 18 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 19 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 20 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 21 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 22 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 23 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 24 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 25 - Numero de acciones: 6 - Reward: 94.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 25\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-1.22 -0.81 -1.39 -1.21 ] [-0.77 -0.48 -0.80 -0.75 ] [-0.39 0.30 -0.44 -0.37 ] [-0.20 -0.14 -0.20 -0.20 ] \n",
      "[-0.79 -0.69 -0.77 2.46 ] [-0.51 -0.45 -0.51 12.08 ] [-0.30 32.14 -0.31 -0.19 ] [-0.10 4.32 -0.11 -0.10 ] \n",
      "[-0.43 -0.49 -0.39 -0.41 ] [-0.41 -0.38 -0.21 1.03 ] [-0.10 -10.10 -0.10 61.60 ] [0.00 91.10 0.00 10.09 ] \n",
      "[-0.40 -0.39 -0.39 -0.48 ] [-0.29 -0.39 -0.39 -10.10 ] [-0.10 0.00 0.00 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -0.81 -0.48  0.30 -0.14\n",
      "x1  2.46 12.08 32.14  4.32\n",
      "x2 -0.39  1.03 61.60 91.10\n",
      "x3 -0.39 -0.29  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  -  -  -\n",
      "x1  1  2  3  -\n",
      "x2  -  -  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner = QLearner,\n",
    "                                        num_episodes = 25,\n",
    "                                        learning_rate = 0.1,\n",
    "                                        discount_factor = 0.9,\n",
    "                                        ratio_exploration = 0.05,\n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list,\n",
    "                    best_episode = best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M5\">5.- SARSA-Learner: Implementación y Ejecución</a>\n",
    "\n",
    "\n",
    "* Recordemos el Pseudocódigo del Algoritmo:\n",
    "\n",
    "\n",
    "<img src=\"./imgs/014_sarsa.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALearner(Learner):\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'SARSA'\n",
    "\n",
    "    def update(self, environment, old_state, action_taken, reward_action_taken, new_state, new_action, is_final_state):\n",
    "        \"\"\"\n",
    "        Método que implementa el algoritmo de aprendizaje SARSA\n",
    "        \n",
    "        :param environment:             Entorno en el que tomar las acciones\n",
    "        :param old_state:               Estado actual\n",
    "        :param action_taken:            Acción a realizar\n",
    "        :param reward_action_taken:     Recompensa obtenida por la acción tomada\n",
    "        :param new_state:               Nuevo estado al que se mueve el agente \n",
    "        :param new_action:              Acción a tomar en el nuevo estado\n",
    "        :param is_final_state:          Boolean. Devuelvel True si el agente llega al estado final; si no, False \n",
    "        \"\"\"\n",
    "        # Obtengo el identificador de la acción\n",
    "        idx_action_taken = list(environment.actions).index(action_taken)\n",
    "\n",
    "        # Obtengo el valor de la acción tomada\n",
    "        actual_q_value_options = self.q_table[old_state[0]][old_state[1]]\n",
    "        actual_q_value = actual_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self.q_table[new_state[0]][new_state[1]]\n",
    "\n",
    "        idx_new_action_taken = list(environment.actions).index(new_action)\n",
    "        future_new_action_q_value = \\\n",
    "            reward_action_taken + self.discount_factor * future_q_value_options[idx_new_action_taken]\n",
    "        if is_final_state:\n",
    "            # Reward máximo si llego a la posición final\n",
    "            future_new_action_q_value = reward_action_taken  # reward máximo\n",
    "\n",
    "        self.q_table[old_state[0]][old_state[1]][idx_action_taken] = \\\n",
    "            actual_q_value + self.learning_rate * (future_new_action_q_value - actual_q_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a corto plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a corto plazo (discount_factor=0.1) pero los movimientos los realiza hacia el estado que mayor recompensa parcial le de y no tiene muy encuenta la recompensa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 115 - Reward: -315.0\n",
      "EPISODIO 2 - Numero de acciones: 28 - Reward: 72.0\n",
      "EPISODIO 3 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 4 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 5 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 6 - Numero de acciones: 40 - Reward: 60.0\n",
      "EPISODIO 7 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 8 - Numero de acciones: 19 - Reward: 81.0\n",
      "EPISODIO 9 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 10 - Numero de acciones: 50 - Reward: 50.0\n",
      "EPISODIO 11 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 12 - Numero de acciones: 28 - Reward: 72.0\n",
      "EPISODIO 13 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 14 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 15 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 16 - Numero de acciones: 50 - Reward: 50.0\n",
      "EPISODIO 17 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 18 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 19 - Numero de acciones: 30 - Reward: 70.0\n",
      "EPISODIO 20 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 21 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 22 - Numero de acciones: 40 - Reward: 60.0\n",
      "EPISODIO 23 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 24 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 25 - Numero de acciones: 22 - Reward: 78.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 3\n",
      "\tNumero de acciones: 7\n",
      "\tReward: 93.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-0.35 -0.35 -0.35 -0.42 ] [-0.27 -0.35 -0.28 -0.27 ] [-0.19 -0.19 -0.19 -0.27 ] [-0.19 -0.27 -0.19 -0.19 ] \n",
      "[-0.27 -0.27 -0.27 -0.27 ] [-0.27 -0.27 -0.19 -0.27 ] [-0.19 -0.19 -0.19 -0.27 ] [-0.19 -0.08 -0.19 -0.19 ] \n",
      "[-0.27 -0.27 -0.27 -0.27 ] [-0.19 -0.27 -0.27 -0.27 ] [-0.19 -10.10 -0.19 -0.09 ] [-0.10 26.83 -0.10 -0.10 ] \n",
      "[-0.27 -0.27 -0.27 -0.27 ] [-0.27 -0.27 -0.27 -10.10 ] [-0.10 -10.10 -0.10 0.00 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -0.35 -0.27 -0.19 -0.19\n",
      "x1 -0.27 -0.19 -0.19 -0.08\n",
      "x2 -0.27 -0.19 -0.09 26.83\n",
      "x3 -0.27 -0.27  0.00  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [1, 3], [1, 3], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  5\n",
      "x2  -  -  -  6\n",
      "x3  -  -  -  7\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner = SARSALearner,\n",
    "                                        num_episodes = 25,\n",
    "                                        learning_rate = 0.1,\n",
    "                                        discount_factor = 0.1,\n",
    "                                        ratio_exploration = 0.05,\n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list,\n",
    "                    best_episode = best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecución con una estrategia a largo plazo\n",
    "\n",
    "* En esta ejecución podemos ver como el agente aprende a interactuar con el entorno siguiendo una estrategia a largo plazo (discount_factor=0.9), realizando movimientos que le den una recompesa final mayor y no asi una recompensa parcial mayor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 23 - Reward: -223.0\n",
      "EPISODIO 2 - Numero de acciones: 34 - Reward: 66.0\n",
      "EPISODIO 3 - Numero de acciones: 50 - Reward: 50.0\n",
      "EPISODIO 4 - Numero de acciones: 35 - Reward: 65.0\n",
      "EPISODIO 5 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 6 - Numero de acciones: 15 - Reward: 85.0\n",
      "EPISODIO 7 - Numero de acciones: 31 - Reward: 69.0\n",
      "EPISODIO 8 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 9 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 10 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 11 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 12 - Numero de acciones: 17 - Reward: 83.0\n",
      "EPISODIO 13 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 14 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 15 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 16 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 17 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 18 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 19 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 20 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 21 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 22 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 23 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 24 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 25 - Numero de acciones: 6 - Reward: 94.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 25\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[-1.32 -1.29 -1.31 -1.32 ] [-0.77 -0.16 -0.80 -0.75 ] [-0.39 0.32 -0.44 -0.44 ] [-0.30 0.28 -0.30 -0.30 ] \n",
      "[-0.81 -0.77 -0.77 -0.42 ] [-0.40 -0.02 -0.43 5.80 ] [-0.21 25.33 -0.22 -0.28 ] [-0.20 10.17 -0.11 0.17 ] \n",
      "[-0.53 -0.47 -0.49 -0.30 ] [-0.31 -0.30 -0.31 7.65 ] [-0.10 -10.10 0.57 60.56 ] [0.00 91.10 -0.10 0.00 ] \n",
      "[-0.39 -0.30 -0.31 -0.39 ] [-0.39 -0.30 -0.29 -10.10 ] [0.00 -10.10 -0.10 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 -1.29 -0.16  0.32  0.28\n",
      "x1 -0.42  5.80 25.33 10.17\n",
      "x2 -0.30  7.65 60.56 91.10\n",
      "x3 -0.30 -0.29  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [2, 2], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  -\n",
      "x2  -  -  4  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "episodes_list, best_episode = run_agent(learner = SARSALearner,\n",
    "                                        num_episodes = 25,\n",
    "                                        learning_rate = 0.1,\n",
    "                                        discount_factor = 0.9,\n",
    "                                        ratio_exploration = 0.05,\n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list,\n",
    "                    best_episode = best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "## <a name=\"M6\">6.- Ejecuciones a realizar por el alumno:</a>\n",
    "\n",
    "\n",
    "* Se piden realizar las siguientes 2 ejecuciones:\n",
    "\n",
    "    1. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 30 episodios con un ratio de exploración del 95% y un factor de descuento de 0.3.\n",
    "    \n",
    "    2. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 150 episodios con un ratio de exploración del 5%, un factor de descuento de 0.9 y un ratio de aprendizaje del 1% (learning_rate=0.01).\n",
    "    \n",
    "    \n",
    "* Para ambas ejecuciones se pide justifucar los resultados obtenidos en el notebook.\n",
    "\n",
    "    ***1: JUSTIFICACIÓN:*** A completar por el alumno\n",
    "    \n",
    "    ***2: JUSTIFICACIÓN:*** A completar por el alumno\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "#### 1. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 30 episodios con un ratio de exploración del 95% y un factor de descuento de 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 145 - Reward: -145.0\n",
      "EPISODIO 2 - Numero de acciones: 24 - Reward: -24.0\n",
      "EPISODIO 3 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 4 - Numero de acciones: 19 - Reward: -119.0\n",
      "EPISODIO 5 - Numero de acciones: 63 - Reward: -663.0\n",
      "EPISODIO 6 - Numero de acciones: 38 - Reward: -238.0\n",
      "EPISODIO 7 - Numero de acciones: 106 - Reward: -206.0\n",
      "EPISODIO 8 - Numero de acciones: 76 - Reward: -76.0\n",
      "EPISODIO 9 - Numero de acciones: 20 - Reward: 80.0\n",
      "EPISODIO 10 - Numero de acciones: 17 - Reward: -17.0\n",
      "EPISODIO 11 - Numero de acciones: 48 - Reward: -48.0\n",
      "EPISODIO 12 - Numero de acciones: 38 - Reward: -38.0\n",
      "EPISODIO 13 - Numero de acciones: 51 - Reward: -251.0\n",
      "EPISODIO 14 - Numero de acciones: 44 - Reward: -44.0\n",
      "EPISODIO 15 - Numero de acciones: 65 - Reward: -65.0\n",
      "EPISODIO 16 - Numero de acciones: 96 - Reward: 4.0\n",
      "EPISODIO 17 - Numero de acciones: 44 - Reward: 56.0\n",
      "EPISODIO 18 - Numero de acciones: 64 - Reward: 36.0\n",
      "EPISODIO 19 - Numero de acciones: 90 - Reward: -190.0\n",
      "EPISODIO 20 - Numero de acciones: 83 - Reward: 17.0\n",
      "EPISODIO 21 - Numero de acciones: 33 - Reward: -233.0\n",
      "EPISODIO 22 - Numero de acciones: 28 - Reward: 72.0\n",
      "EPISODIO 23 - Numero de acciones: 16 - Reward: -16.0\n",
      "EPISODIO 24 - Numero de acciones: 10 - Reward: -110.0\n",
      "EPISODIO 25 - Numero de acciones: 57 - Reward: 43.0\n",
      "EPISODIO 26 - Numero de acciones: 273 - Reward: -1773.0\n",
      "EPISODIO 27 - Numero de acciones: 37 - Reward: 63.0\n",
      "EPISODIO 28 - Numero de acciones: 13 - Reward: 87.0\n",
      "EPISODIO 29 - Numero de acciones: 56 - Reward: 44.0\n",
      "EPISODIO 30 - Numero de acciones: 63 - Reward: -163.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 3\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[0.00 -0.27 -0.27 -0.35 ] [-0.36 -0.28 -0.28 -0.20 ] [-0.43 -0.28 -0.19 -0.67 ] [-0.58 -0.43 -0.55 -0.56 ] \n",
      "[-0.10 -0.28 -0.20 -0.19 ] [-0.19 -0.10 0.00 -0.48 ] [-0.43 -0.42 0.00 -0.28 ] [-0.29 -0.27 -0.27 -0.10 ] \n",
      "[-0.28 -0.19 -0.44 -0.56 ] [-0.35 -0.35 -0.49 -0.19 ] [-0.10 -10.10 -0.43 -0.10 ] [-0.11 18.81 -0.10 -0.10 ] \n",
      "[-0.28 -0.10 -0.28 -0.20 ] [-0.29 -0.35 -0.28 -10.10 ] [0.00 0.00 -0.40 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0  0.00 -0.20 -0.19 -0.43\n",
      "x1 -0.10  0.00  0.00 -0.10\n",
      "x2 -0.19 -0.19 -0.10 18.81\n",
      "x3 -0.10 -0.28  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[0, 1], [1, 1], [1, 2], [1, 3], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  1  -  -\n",
      "x1  -  2  3  4\n",
      "x2  -  -  -  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "episodes_list, best_episode = run_agent(learner = SARSALearner,\n",
    "                                        num_episodes = 30,\n",
    "                                        learning_rate = 0.1,\n",
    "                                        discount_factor = 0.3,\n",
    "                                        ratio_exploration = 0.95,\n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list,\n",
    "                    best_episode = best_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### 2. Ejecutar con el algoritmo de Q-Learning o SARSA (uno de los dos) 150 episodios con un ratio de exploración del 5%, un factor de descuento de 0.9 y un ratio de aprendizaje del 1% (learning_rate=0.01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPISODIO 1 - Numero de acciones: 37 - Reward: 63.0\n",
      "EPISODIO 2 - Numero de acciones: 20 - Reward: 80.0\n",
      "EPISODIO 3 - Numero de acciones: 12 - Reward: -112.0\n",
      "EPISODIO 4 - Numero de acciones: 16 - Reward: -116.0\n",
      "EPISODIO 5 - Numero de acciones: 29 - Reward: 71.0\n",
      "EPISODIO 6 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 7 - Numero de acciones: 11 - Reward: 89.0\n",
      "EPISODIO 8 - Numero de acciones: 14 - Reward: 86.0\n",
      "EPISODIO 9 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 10 - Numero de acciones: 16 - Reward: 84.0\n",
      "EPISODIO 11 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 12 - Numero de acciones: 18 - Reward: 82.0\n",
      "EPISODIO 13 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 14 - Numero de acciones: 20 - Reward: 80.0\n",
      "EPISODIO 15 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 16 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 17 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 18 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 19 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 20 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 21 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 22 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 23 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 24 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 25 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 26 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 27 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 28 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 29 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 30 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 31 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 32 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 33 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 34 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 35 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 36 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 37 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 38 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 39 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 40 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 41 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 42 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 43 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 44 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 45 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 46 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 47 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 48 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 49 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 50 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 51 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 52 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 53 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 54 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 55 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 56 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 57 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 58 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 59 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 60 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 61 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 62 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 63 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 64 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 65 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 66 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 67 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 68 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 69 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 70 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 71 - Numero de acciones: 10 - Reward: 90.0\n",
      "EPISODIO 72 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 73 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 74 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 75 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 76 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 77 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 78 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 79 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 80 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 81 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 82 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 83 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 84 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 85 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 86 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 87 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 88 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 89 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 90 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 91 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 92 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 93 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 94 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 95 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 96 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 97 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 98 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 99 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 100 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 101 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 102 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 103 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 104 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 105 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 106 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 107 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 108 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 109 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 110 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 111 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 112 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 113 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 114 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 115 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 116 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 117 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 118 - Numero de acciones: 7 - Reward: 93.0\n",
      "EPISODIO 119 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 120 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 121 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 122 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 123 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 124 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 125 - Numero de acciones: 9 - Reward: 91.0\n",
      "EPISODIO 126 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 127 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 128 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 129 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 130 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 131 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 132 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 133 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 134 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 135 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 136 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 137 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 138 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 139 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 140 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 141 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 142 - Numero de acciones: 8 - Reward: 92.0\n",
      "EPISODIO 143 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 144 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 145 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 146 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 147 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 148 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 149 - Numero de acciones: 6 - Reward: 94.0\n",
      "EPISODIO 150 - Numero de acciones: 7 - Reward: 93.0\n",
      "\n",
      "MEJOR (ÚLTIMO) EPISODIO:\n",
      "\\EPISODIO 149\n",
      "\tNumero de acciones: 6\n",
      "\tReward: 94.0\n",
      "\n",
      "Q_TABLE:\n",
      "Actions = Arriba - Abajo - Izquierda - Derecha\n",
      "[2.80 46.03 4.03 -1.10 ] [-0.58 -0.19 -0.58 -0.60 ] [-0.30 11.61 -0.33 -0.27 ] [-0.10 7.38 -0.11 -0.20 ] \n",
      "[9.92 -0.62 4.24 52.69 ] [-0.33 0.38 6.64 64.73 ] [0.24 3.78 -0.11 71.34 ] [-0.10 87.97 0.03 -0.10 ] \n",
      "[5.79 -0.44 -0.48 -0.46 ] [-0.29 -0.30 -0.31 10.85 ] [-0.10 -10.10 -0.10 57.54 ] [6.79 99.00 4.99 16.27 ] \n",
      "[-0.41 -0.39 -0.39 -0.37 ] [-0.30 -0.20 -0.31 -10.10 ] [-0.10 -19.19 0.00 9.90 ] [0.00 0.00 0.00 0.00 ] \n",
      "\n",
      "BEST Q_TABLE VALUES:\n",
      "      y0    y1    y2    y3\n",
      "x0 46.03 -0.19 11.61  7.38\n",
      "x1 52.69 64.73 71.34 87.97\n",
      "x2  5.79 10.85 57.54 99.00\n",
      "x3 -0.37 -0.20  9.90  0.00\n",
      "\n",
      "Pasos: \n",
      "   [[1, 0], [1, 1], [1, 2], [1, 3], [2, 3], [3, 3]]\n",
      "\n",
      "PATH:\n",
      "   y0 y1 y2 y3\n",
      "x0  0  -  -  -\n",
      "x1  1  2  3  4\n",
      "x2  -  -  -  5\n",
      "x3  -  -  -  6\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "episodes_list, best_episode = run_agent(learner = SARSALearner,\n",
    "                                        num_episodes = 150,\n",
    "                                        learning_rate = 0.1,\n",
    "                                        discount_factor = 0.9,\n",
    "                                        ratio_exploration = 0.05,\n",
    "                                        verbose = True)\n",
    "\n",
    "print_process_info(episodes_list = episodes_list,\n",
    "                    best_episode = best_episode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
